// coverage:ignore-file
// GENERATED CODE - DO NOT MODIFY BY HAND
// ignore_for_file: type=lint
// ignore_for_file: unused_element, deprecated_member_use, deprecated_member_use_from_same_package, use_function_type_syntax_for_parameters, unnecessary_const, avoid_init_to_null, invalid_override_different_default_values_named, prefer_expression_function_bodies, annotate_overrides, invalid_annotation_target, unnecessary_question_mark

part of 'error.dart';

// **************************************************************************
// FreezedGenerator
// **************************************************************************

T _$identity<T>(T value) => value;

final _privateConstructorUsedError = UnsupportedError(
    'It seems like you constructed your class using `MyClass._()`. This constructor is only meant to be used by freezed and you are not supposed to need it nor use it.\nPlease check the documentation here for more information: https://github.com/rrousselGit/freezed#adding-getters-and-methods-to-our-models');

/// @nodoc
mixin _$InferenceError {
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) =>
      throw _privateConstructorUsedError;
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) =>
      throw _privateConstructorUsedError;
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) =>
      throw _privateConstructorUsedError;
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) =>
      throw _privateConstructorUsedError;
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) =>
      throw _privateConstructorUsedError;
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) =>
      throw _privateConstructorUsedError;
}

/// @nodoc
abstract class $InferenceErrorCopyWith<$Res> {
  factory $InferenceErrorCopyWith(
          InferenceError value, $Res Function(InferenceError) then) =
      _$InferenceErrorCopyWithImpl<$Res, InferenceError>;
}

/// @nodoc
class _$InferenceErrorCopyWithImpl<$Res, $Val extends InferenceError>
    implements $InferenceErrorCopyWith<$Res> {
  _$InferenceErrorCopyWithImpl(this._value, this._then);

  // ignore: unused_field
  final $Val _value;
  // ignore: unused_field
  final $Res Function($Val) _then;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
}

/// @nodoc
abstract class _$$InferenceError_ModelLoadImplCopyWith<$Res> {
  factory _$$InferenceError_ModelLoadImplCopyWith(
          _$InferenceError_ModelLoadImpl value,
          $Res Function(_$InferenceError_ModelLoadImpl) then) =
      __$$InferenceError_ModelLoadImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_ModelLoadImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res, _$InferenceError_ModelLoadImpl>
    implements _$$InferenceError_ModelLoadImplCopyWith<$Res> {
  __$$InferenceError_ModelLoadImplCopyWithImpl(
      _$InferenceError_ModelLoadImpl _value,
      $Res Function(_$InferenceError_ModelLoadImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_ModelLoadImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_ModelLoadImpl extends InferenceError_ModelLoad {
  const _$InferenceError_ModelLoadImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.modelLoad(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_ModelLoadImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_ModelLoadImplCopyWith<_$InferenceError_ModelLoadImpl>
      get copyWith => __$$InferenceError_ModelLoadImplCopyWithImpl<
          _$InferenceError_ModelLoadImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return modelLoad(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return modelLoad?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (modelLoad != null) {
      return modelLoad(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return modelLoad(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return modelLoad?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (modelLoad != null) {
      return modelLoad(this);
    }
    return orElse();
  }
}

abstract class InferenceError_ModelLoad extends InferenceError {
  const factory InferenceError_ModelLoad(final String field0) =
      _$InferenceError_ModelLoadImpl;
  const InferenceError_ModelLoad._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_ModelLoadImplCopyWith<_$InferenceError_ModelLoadImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_PredictionImplCopyWith<$Res> {
  factory _$$InferenceError_PredictionImplCopyWith(
          _$InferenceError_PredictionImpl value,
          $Res Function(_$InferenceError_PredictionImpl) then) =
      __$$InferenceError_PredictionImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_PredictionImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res, _$InferenceError_PredictionImpl>
    implements _$$InferenceError_PredictionImplCopyWith<$Res> {
  __$$InferenceError_PredictionImplCopyWithImpl(
      _$InferenceError_PredictionImpl _value,
      $Res Function(_$InferenceError_PredictionImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_PredictionImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_PredictionImpl extends InferenceError_Prediction {
  const _$InferenceError_PredictionImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.prediction(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_PredictionImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_PredictionImplCopyWith<_$InferenceError_PredictionImpl>
      get copyWith => __$$InferenceError_PredictionImplCopyWithImpl<
          _$InferenceError_PredictionImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return prediction(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return prediction?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (prediction != null) {
      return prediction(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return prediction(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return prediction?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (prediction != null) {
      return prediction(this);
    }
    return orElse();
  }
}

abstract class InferenceError_Prediction extends InferenceError {
  const factory InferenceError_Prediction(final String field0) =
      _$InferenceError_PredictionImpl;
  const InferenceError_Prediction._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_PredictionImplCopyWith<_$InferenceError_PredictionImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_UnsupportedFormatImplCopyWith<$Res> {
  factory _$$InferenceError_UnsupportedFormatImplCopyWith(
          _$InferenceError_UnsupportedFormatImpl value,
          $Res Function(_$InferenceError_UnsupportedFormatImpl) then) =
      __$$InferenceError_UnsupportedFormatImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_UnsupportedFormatImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_UnsupportedFormatImpl>
    implements _$$InferenceError_UnsupportedFormatImplCopyWith<$Res> {
  __$$InferenceError_UnsupportedFormatImplCopyWithImpl(
      _$InferenceError_UnsupportedFormatImpl _value,
      $Res Function(_$InferenceError_UnsupportedFormatImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_UnsupportedFormatImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_UnsupportedFormatImpl
    extends InferenceError_UnsupportedFormat {
  const _$InferenceError_UnsupportedFormatImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.unsupportedFormat(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_UnsupportedFormatImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_UnsupportedFormatImplCopyWith<
          _$InferenceError_UnsupportedFormatImpl>
      get copyWith => __$$InferenceError_UnsupportedFormatImplCopyWithImpl<
          _$InferenceError_UnsupportedFormatImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return unsupportedFormat(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return unsupportedFormat?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (unsupportedFormat != null) {
      return unsupportedFormat(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return unsupportedFormat(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return unsupportedFormat?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (unsupportedFormat != null) {
      return unsupportedFormat(this);
    }
    return orElse();
  }
}

abstract class InferenceError_UnsupportedFormat extends InferenceError {
  const factory InferenceError_UnsupportedFormat(final String field0) =
      _$InferenceError_UnsupportedFormatImpl;
  const InferenceError_UnsupportedFormat._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_UnsupportedFormatImplCopyWith<
          _$InferenceError_UnsupportedFormatImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_InvalidShapeImplCopyWith<$Res> {
  factory _$$InferenceError_InvalidShapeImplCopyWith(
          _$InferenceError_InvalidShapeImpl value,
          $Res Function(_$InferenceError_InvalidShapeImpl) then) =
      __$$InferenceError_InvalidShapeImplCopyWithImpl<$Res>;
  @useResult
  $Res call({Uint64List expected, Uint64List actual});
}

/// @nodoc
class __$$InferenceError_InvalidShapeImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_InvalidShapeImpl>
    implements _$$InferenceError_InvalidShapeImplCopyWith<$Res> {
  __$$InferenceError_InvalidShapeImplCopyWithImpl(
      _$InferenceError_InvalidShapeImpl _value,
      $Res Function(_$InferenceError_InvalidShapeImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? expected = null,
    Object? actual = null,
  }) {
    return _then(_$InferenceError_InvalidShapeImpl(
      expected: null == expected
          ? _value.expected
          : expected // ignore: cast_nullable_to_non_nullable
              as Uint64List,
      actual: null == actual
          ? _value.actual
          : actual // ignore: cast_nullable_to_non_nullable
              as Uint64List,
    ));
  }
}

/// @nodoc

class _$InferenceError_InvalidShapeImpl extends InferenceError_InvalidShape {
  const _$InferenceError_InvalidShapeImpl(
      {required this.expected, required this.actual})
      : super._();

  @override
  final Uint64List expected;
  @override
  final Uint64List actual;

  @override
  String toString() {
    return 'InferenceError.invalidShape(expected: $expected, actual: $actual)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_InvalidShapeImpl &&
            const DeepCollectionEquality().equals(other.expected, expected) &&
            const DeepCollectionEquality().equals(other.actual, actual));
  }

  @override
  int get hashCode => Object.hash(
      runtimeType,
      const DeepCollectionEquality().hash(expected),
      const DeepCollectionEquality().hash(actual));

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_InvalidShapeImplCopyWith<_$InferenceError_InvalidShapeImpl>
      get copyWith => __$$InferenceError_InvalidShapeImplCopyWithImpl<
          _$InferenceError_InvalidShapeImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return invalidShape(expected, actual);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return invalidShape?.call(expected, actual);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (invalidShape != null) {
      return invalidShape(expected, actual);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return invalidShape(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return invalidShape?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (invalidShape != null) {
      return invalidShape(this);
    }
    return orElse();
  }
}

abstract class InferenceError_InvalidShape extends InferenceError {
  const factory InferenceError_InvalidShape(
      {required final Uint64List expected,
      required final Uint64List actual}) = _$InferenceError_InvalidShapeImpl;
  const InferenceError_InvalidShape._() : super._();

  Uint64List get expected;
  Uint64List get actual;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_InvalidShapeImplCopyWith<_$InferenceError_InvalidShapeImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_InvalidTensorDataImplCopyWith<$Res> {
  factory _$$InferenceError_InvalidTensorDataImplCopyWith(
          _$InferenceError_InvalidTensorDataImpl value,
          $Res Function(_$InferenceError_InvalidTensorDataImpl) then) =
      __$$InferenceError_InvalidTensorDataImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_InvalidTensorDataImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_InvalidTensorDataImpl>
    implements _$$InferenceError_InvalidTensorDataImplCopyWith<$Res> {
  __$$InferenceError_InvalidTensorDataImplCopyWithImpl(
      _$InferenceError_InvalidTensorDataImpl _value,
      $Res Function(_$InferenceError_InvalidTensorDataImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_InvalidTensorDataImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_InvalidTensorDataImpl
    extends InferenceError_InvalidTensorData {
  const _$InferenceError_InvalidTensorDataImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.invalidTensorData(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_InvalidTensorDataImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_InvalidTensorDataImplCopyWith<
          _$InferenceError_InvalidTensorDataImpl>
      get copyWith => __$$InferenceError_InvalidTensorDataImplCopyWithImpl<
          _$InferenceError_InvalidTensorDataImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return invalidTensorData(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return invalidTensorData?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (invalidTensorData != null) {
      return invalidTensorData(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return invalidTensorData(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return invalidTensorData?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (invalidTensorData != null) {
      return invalidTensorData(this);
    }
    return orElse();
  }
}

abstract class InferenceError_InvalidTensorData extends InferenceError {
  const factory InferenceError_InvalidTensorData(final String field0) =
      _$InferenceError_InvalidTensorDataImpl;
  const InferenceError_InvalidTensorData._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_InvalidTensorDataImplCopyWith<
          _$InferenceError_InvalidTensorDataImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_EngineImplCopyWith<$Res> {
  factory _$$InferenceError_EngineImplCopyWith(
          _$InferenceError_EngineImpl value,
          $Res Function(_$InferenceError_EngineImpl) then) =
      __$$InferenceError_EngineImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_EngineImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res, _$InferenceError_EngineImpl>
    implements _$$InferenceError_EngineImplCopyWith<$Res> {
  __$$InferenceError_EngineImplCopyWithImpl(_$InferenceError_EngineImpl _value,
      $Res Function(_$InferenceError_EngineImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_EngineImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_EngineImpl extends InferenceError_Engine {
  const _$InferenceError_EngineImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.engine(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_EngineImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_EngineImplCopyWith<_$InferenceError_EngineImpl>
      get copyWith => __$$InferenceError_EngineImplCopyWithImpl<
          _$InferenceError_EngineImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return engine(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return engine?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (engine != null) {
      return engine(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return engine(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return engine?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (engine != null) {
      return engine(this);
    }
    return orElse();
  }
}

abstract class InferenceError_Engine extends InferenceError {
  const factory InferenceError_Engine(final String field0) =
      _$InferenceError_EngineImpl;
  const InferenceError_Engine._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_EngineImplCopyWith<_$InferenceError_EngineImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_IoImplCopyWith<$Res> {
  factory _$$InferenceError_IoImplCopyWith(_$InferenceError_IoImpl value,
          $Res Function(_$InferenceError_IoImpl) then) =
      __$$InferenceError_IoImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_IoImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res, _$InferenceError_IoImpl>
    implements _$$InferenceError_IoImplCopyWith<$Res> {
  __$$InferenceError_IoImplCopyWithImpl(_$InferenceError_IoImpl _value,
      $Res Function(_$InferenceError_IoImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_IoImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_IoImpl extends InferenceError_Io {
  const _$InferenceError_IoImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.io(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_IoImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_IoImplCopyWith<_$InferenceError_IoImpl> get copyWith =>
      __$$InferenceError_IoImplCopyWithImpl<_$InferenceError_IoImpl>(
          this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return io(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return io?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (io != null) {
      return io(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return io(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return io?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (io != null) {
      return io(this);
    }
    return orElse();
  }
}

abstract class InferenceError_Io extends InferenceError {
  const factory InferenceError_Io(final String field0) =
      _$InferenceError_IoImpl;
  const InferenceError_Io._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_IoImplCopyWith<_$InferenceError_IoImpl> get copyWith =>
      throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_SerializationImplCopyWith<$Res> {
  factory _$$InferenceError_SerializationImplCopyWith(
          _$InferenceError_SerializationImpl value,
          $Res Function(_$InferenceError_SerializationImpl) then) =
      __$$InferenceError_SerializationImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_SerializationImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_SerializationImpl>
    implements _$$InferenceError_SerializationImplCopyWith<$Res> {
  __$$InferenceError_SerializationImplCopyWithImpl(
      _$InferenceError_SerializationImpl _value,
      $Res Function(_$InferenceError_SerializationImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_SerializationImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_SerializationImpl extends InferenceError_Serialization {
  const _$InferenceError_SerializationImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.serialization(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_SerializationImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_SerializationImplCopyWith<
          _$InferenceError_SerializationImpl>
      get copyWith => __$$InferenceError_SerializationImplCopyWithImpl<
          _$InferenceError_SerializationImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return serialization(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return serialization?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (serialization != null) {
      return serialization(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return serialization(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return serialization?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (serialization != null) {
      return serialization(this);
    }
    return orElse();
  }
}

abstract class InferenceError_Serialization extends InferenceError {
  const factory InferenceError_Serialization(final String field0) =
      _$InferenceError_SerializationImpl;
  const InferenceError_Serialization._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_SerializationImplCopyWith<
          _$InferenceError_SerializationImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_ConfigurationImplCopyWith<$Res> {
  factory _$$InferenceError_ConfigurationImplCopyWith(
          _$InferenceError_ConfigurationImpl value,
          $Res Function(_$InferenceError_ConfigurationImpl) then) =
      __$$InferenceError_ConfigurationImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_ConfigurationImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_ConfigurationImpl>
    implements _$$InferenceError_ConfigurationImplCopyWith<$Res> {
  __$$InferenceError_ConfigurationImplCopyWithImpl(
      _$InferenceError_ConfigurationImpl _value,
      $Res Function(_$InferenceError_ConfigurationImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_ConfigurationImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_ConfigurationImpl extends InferenceError_Configuration {
  const _$InferenceError_ConfigurationImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.configuration(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_ConfigurationImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_ConfigurationImplCopyWith<
          _$InferenceError_ConfigurationImpl>
      get copyWith => __$$InferenceError_ConfigurationImplCopyWithImpl<
          _$InferenceError_ConfigurationImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return configuration(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return configuration?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (configuration != null) {
      return configuration(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return configuration(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return configuration?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (configuration != null) {
      return configuration(this);
    }
    return orElse();
  }
}

abstract class InferenceError_Configuration extends InferenceError {
  const factory InferenceError_Configuration(final String field0) =
      _$InferenceError_ConfigurationImpl;
  const InferenceError_Configuration._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_ConfigurationImplCopyWith<
          _$InferenceError_ConfigurationImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_ResourceNotFoundImplCopyWith<$Res> {
  factory _$$InferenceError_ResourceNotFoundImplCopyWith(
          _$InferenceError_ResourceNotFoundImpl value,
          $Res Function(_$InferenceError_ResourceNotFoundImpl) then) =
      __$$InferenceError_ResourceNotFoundImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_ResourceNotFoundImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_ResourceNotFoundImpl>
    implements _$$InferenceError_ResourceNotFoundImplCopyWith<$Res> {
  __$$InferenceError_ResourceNotFoundImplCopyWithImpl(
      _$InferenceError_ResourceNotFoundImpl _value,
      $Res Function(_$InferenceError_ResourceNotFoundImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_ResourceNotFoundImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_ResourceNotFoundImpl
    extends InferenceError_ResourceNotFound {
  const _$InferenceError_ResourceNotFoundImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.resourceNotFound(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_ResourceNotFoundImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_ResourceNotFoundImplCopyWith<
          _$InferenceError_ResourceNotFoundImpl>
      get copyWith => __$$InferenceError_ResourceNotFoundImplCopyWithImpl<
          _$InferenceError_ResourceNotFoundImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return resourceNotFound(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return resourceNotFound?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (resourceNotFound != null) {
      return resourceNotFound(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return resourceNotFound(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return resourceNotFound?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (resourceNotFound != null) {
      return resourceNotFound(this);
    }
    return orElse();
  }
}

abstract class InferenceError_ResourceNotFound extends InferenceError {
  const factory InferenceError_ResourceNotFound(final String field0) =
      _$InferenceError_ResourceNotFoundImpl;
  const InferenceError_ResourceNotFound._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_ResourceNotFoundImplCopyWith<
          _$InferenceError_ResourceNotFoundImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_MemoryAllocationImplCopyWith<$Res> {
  factory _$$InferenceError_MemoryAllocationImplCopyWith(
          _$InferenceError_MemoryAllocationImpl value,
          $Res Function(_$InferenceError_MemoryAllocationImpl) then) =
      __$$InferenceError_MemoryAllocationImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_MemoryAllocationImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_MemoryAllocationImpl>
    implements _$$InferenceError_MemoryAllocationImplCopyWith<$Res> {
  __$$InferenceError_MemoryAllocationImplCopyWithImpl(
      _$InferenceError_MemoryAllocationImpl _value,
      $Res Function(_$InferenceError_MemoryAllocationImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_MemoryAllocationImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_MemoryAllocationImpl
    extends InferenceError_MemoryAllocation {
  const _$InferenceError_MemoryAllocationImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.memoryAllocation(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_MemoryAllocationImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_MemoryAllocationImplCopyWith<
          _$InferenceError_MemoryAllocationImpl>
      get copyWith => __$$InferenceError_MemoryAllocationImplCopyWithImpl<
          _$InferenceError_MemoryAllocationImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return memoryAllocation(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return memoryAllocation?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (memoryAllocation != null) {
      return memoryAllocation(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return memoryAllocation(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return memoryAllocation?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (memoryAllocation != null) {
      return memoryAllocation(this);
    }
    return orElse();
  }
}

abstract class InferenceError_MemoryAllocation extends InferenceError {
  const factory InferenceError_MemoryAllocation(final String field0) =
      _$InferenceError_MemoryAllocationImpl;
  const InferenceError_MemoryAllocation._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_MemoryAllocationImplCopyWith<
          _$InferenceError_MemoryAllocationImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_ThreadPoolImplCopyWith<$Res> {
  factory _$$InferenceError_ThreadPoolImplCopyWith(
          _$InferenceError_ThreadPoolImpl value,
          $Res Function(_$InferenceError_ThreadPoolImpl) then) =
      __$$InferenceError_ThreadPoolImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_ThreadPoolImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res, _$InferenceError_ThreadPoolImpl>
    implements _$$InferenceError_ThreadPoolImplCopyWith<$Res> {
  __$$InferenceError_ThreadPoolImplCopyWithImpl(
      _$InferenceError_ThreadPoolImpl _value,
      $Res Function(_$InferenceError_ThreadPoolImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_ThreadPoolImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_ThreadPoolImpl extends InferenceError_ThreadPool {
  const _$InferenceError_ThreadPoolImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.threadPool(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_ThreadPoolImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_ThreadPoolImplCopyWith<_$InferenceError_ThreadPoolImpl>
      get copyWith => __$$InferenceError_ThreadPoolImplCopyWithImpl<
          _$InferenceError_ThreadPoolImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return threadPool(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return threadPool?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (threadPool != null) {
      return threadPool(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return threadPool(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return threadPool?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (threadPool != null) {
      return threadPool(this);
    }
    return orElse();
  }
}

abstract class InferenceError_ThreadPool extends InferenceError {
  const factory InferenceError_ThreadPool(final String field0) =
      _$InferenceError_ThreadPoolImpl;
  const InferenceError_ThreadPool._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_ThreadPoolImplCopyWith<_$InferenceError_ThreadPoolImpl>
      get copyWith => throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_GpuImplCopyWith<$Res> {
  factory _$$InferenceError_GpuImplCopyWith(_$InferenceError_GpuImpl value,
          $Res Function(_$InferenceError_GpuImpl) then) =
      __$$InferenceError_GpuImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_GpuImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res, _$InferenceError_GpuImpl>
    implements _$$InferenceError_GpuImplCopyWith<$Res> {
  __$$InferenceError_GpuImplCopyWithImpl(_$InferenceError_GpuImpl _value,
      $Res Function(_$InferenceError_GpuImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_GpuImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_GpuImpl extends InferenceError_Gpu {
  const _$InferenceError_GpuImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.gpu(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_GpuImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_GpuImplCopyWith<_$InferenceError_GpuImpl> get copyWith =>
      __$$InferenceError_GpuImplCopyWithImpl<_$InferenceError_GpuImpl>(
          this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return gpu(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return gpu?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (gpu != null) {
      return gpu(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return gpu(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return gpu?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (gpu != null) {
      return gpu(this);
    }
    return orElse();
  }
}

abstract class InferenceError_Gpu extends InferenceError {
  const factory InferenceError_Gpu(final String field0) =
      _$InferenceError_GpuImpl;
  const InferenceError_Gpu._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_GpuImplCopyWith<_$InferenceError_GpuImpl> get copyWith =>
      throw _privateConstructorUsedError;
}

/// @nodoc
abstract class _$$InferenceError_FormatDetectionImplCopyWith<$Res> {
  factory _$$InferenceError_FormatDetectionImplCopyWith(
          _$InferenceError_FormatDetectionImpl value,
          $Res Function(_$InferenceError_FormatDetectionImpl) then) =
      __$$InferenceError_FormatDetectionImplCopyWithImpl<$Res>;
  @useResult
  $Res call({String field0});
}

/// @nodoc
class __$$InferenceError_FormatDetectionImplCopyWithImpl<$Res>
    extends _$InferenceErrorCopyWithImpl<$Res,
        _$InferenceError_FormatDetectionImpl>
    implements _$$InferenceError_FormatDetectionImplCopyWith<$Res> {
  __$$InferenceError_FormatDetectionImplCopyWithImpl(
      _$InferenceError_FormatDetectionImpl _value,
      $Res Function(_$InferenceError_FormatDetectionImpl) _then)
      : super(_value, _then);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @pragma('vm:prefer-inline')
  @override
  $Res call({
    Object? field0 = null,
  }) {
    return _then(_$InferenceError_FormatDetectionImpl(
      null == field0
          ? _value.field0
          : field0 // ignore: cast_nullable_to_non_nullable
              as String,
    ));
  }
}

/// @nodoc

class _$InferenceError_FormatDetectionImpl
    extends InferenceError_FormatDetection {
  const _$InferenceError_FormatDetectionImpl(this.field0) : super._();

  @override
  final String field0;

  @override
  String toString() {
    return 'InferenceError.formatDetection(field0: $field0)';
  }

  @override
  bool operator ==(Object other) {
    return identical(this, other) ||
        (other.runtimeType == runtimeType &&
            other is _$InferenceError_FormatDetectionImpl &&
            (identical(other.field0, field0) || other.field0 == field0));
  }

  @override
  int get hashCode => Object.hash(runtimeType, field0);

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  @override
  @pragma('vm:prefer-inline')
  _$$InferenceError_FormatDetectionImplCopyWith<
          _$InferenceError_FormatDetectionImpl>
      get copyWith => __$$InferenceError_FormatDetectionImplCopyWithImpl<
          _$InferenceError_FormatDetectionImpl>(this, _$identity);

  @override
  @optionalTypeArgs
  TResult when<TResult extends Object?>({
    required TResult Function(String field0) modelLoad,
    required TResult Function(String field0) prediction,
    required TResult Function(String field0) unsupportedFormat,
    required TResult Function(Uint64List expected, Uint64List actual)
        invalidShape,
    required TResult Function(String field0) invalidTensorData,
    required TResult Function(String field0) engine,
    required TResult Function(String field0) io,
    required TResult Function(String field0) serialization,
    required TResult Function(String field0) configuration,
    required TResult Function(String field0) resourceNotFound,
    required TResult Function(String field0) memoryAllocation,
    required TResult Function(String field0) threadPool,
    required TResult Function(String field0) gpu,
    required TResult Function(String field0) formatDetection,
  }) {
    return formatDetection(field0);
  }

  @override
  @optionalTypeArgs
  TResult? whenOrNull<TResult extends Object?>({
    TResult? Function(String field0)? modelLoad,
    TResult? Function(String field0)? prediction,
    TResult? Function(String field0)? unsupportedFormat,
    TResult? Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult? Function(String field0)? invalidTensorData,
    TResult? Function(String field0)? engine,
    TResult? Function(String field0)? io,
    TResult? Function(String field0)? serialization,
    TResult? Function(String field0)? configuration,
    TResult? Function(String field0)? resourceNotFound,
    TResult? Function(String field0)? memoryAllocation,
    TResult? Function(String field0)? threadPool,
    TResult? Function(String field0)? gpu,
    TResult? Function(String field0)? formatDetection,
  }) {
    return formatDetection?.call(field0);
  }

  @override
  @optionalTypeArgs
  TResult maybeWhen<TResult extends Object?>({
    TResult Function(String field0)? modelLoad,
    TResult Function(String field0)? prediction,
    TResult Function(String field0)? unsupportedFormat,
    TResult Function(Uint64List expected, Uint64List actual)? invalidShape,
    TResult Function(String field0)? invalidTensorData,
    TResult Function(String field0)? engine,
    TResult Function(String field0)? io,
    TResult Function(String field0)? serialization,
    TResult Function(String field0)? configuration,
    TResult Function(String field0)? resourceNotFound,
    TResult Function(String field0)? memoryAllocation,
    TResult Function(String field0)? threadPool,
    TResult Function(String field0)? gpu,
    TResult Function(String field0)? formatDetection,
    required TResult orElse(),
  }) {
    if (formatDetection != null) {
      return formatDetection(field0);
    }
    return orElse();
  }

  @override
  @optionalTypeArgs
  TResult map<TResult extends Object?>({
    required TResult Function(InferenceError_ModelLoad value) modelLoad,
    required TResult Function(InferenceError_Prediction value) prediction,
    required TResult Function(InferenceError_UnsupportedFormat value)
        unsupportedFormat,
    required TResult Function(InferenceError_InvalidShape value) invalidShape,
    required TResult Function(InferenceError_InvalidTensorData value)
        invalidTensorData,
    required TResult Function(InferenceError_Engine value) engine,
    required TResult Function(InferenceError_Io value) io,
    required TResult Function(InferenceError_Serialization value) serialization,
    required TResult Function(InferenceError_Configuration value) configuration,
    required TResult Function(InferenceError_ResourceNotFound value)
        resourceNotFound,
    required TResult Function(InferenceError_MemoryAllocation value)
        memoryAllocation,
    required TResult Function(InferenceError_ThreadPool value) threadPool,
    required TResult Function(InferenceError_Gpu value) gpu,
    required TResult Function(InferenceError_FormatDetection value)
        formatDetection,
  }) {
    return formatDetection(this);
  }

  @override
  @optionalTypeArgs
  TResult? mapOrNull<TResult extends Object?>({
    TResult? Function(InferenceError_ModelLoad value)? modelLoad,
    TResult? Function(InferenceError_Prediction value)? prediction,
    TResult? Function(InferenceError_UnsupportedFormat value)?
        unsupportedFormat,
    TResult? Function(InferenceError_InvalidShape value)? invalidShape,
    TResult? Function(InferenceError_InvalidTensorData value)?
        invalidTensorData,
    TResult? Function(InferenceError_Engine value)? engine,
    TResult? Function(InferenceError_Io value)? io,
    TResult? Function(InferenceError_Serialization value)? serialization,
    TResult? Function(InferenceError_Configuration value)? configuration,
    TResult? Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult? Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult? Function(InferenceError_ThreadPool value)? threadPool,
    TResult? Function(InferenceError_Gpu value)? gpu,
    TResult? Function(InferenceError_FormatDetection value)? formatDetection,
  }) {
    return formatDetection?.call(this);
  }

  @override
  @optionalTypeArgs
  TResult maybeMap<TResult extends Object?>({
    TResult Function(InferenceError_ModelLoad value)? modelLoad,
    TResult Function(InferenceError_Prediction value)? prediction,
    TResult Function(InferenceError_UnsupportedFormat value)? unsupportedFormat,
    TResult Function(InferenceError_InvalidShape value)? invalidShape,
    TResult Function(InferenceError_InvalidTensorData value)? invalidTensorData,
    TResult Function(InferenceError_Engine value)? engine,
    TResult Function(InferenceError_Io value)? io,
    TResult Function(InferenceError_Serialization value)? serialization,
    TResult Function(InferenceError_Configuration value)? configuration,
    TResult Function(InferenceError_ResourceNotFound value)? resourceNotFound,
    TResult Function(InferenceError_MemoryAllocation value)? memoryAllocation,
    TResult Function(InferenceError_ThreadPool value)? threadPool,
    TResult Function(InferenceError_Gpu value)? gpu,
    TResult Function(InferenceError_FormatDetection value)? formatDetection,
    required TResult orElse(),
  }) {
    if (formatDetection != null) {
      return formatDetection(this);
    }
    return orElse();
  }
}

abstract class InferenceError_FormatDetection extends InferenceError {
  const factory InferenceError_FormatDetection(final String field0) =
      _$InferenceError_FormatDetectionImpl;
  const InferenceError_FormatDetection._() : super._();

  String get field0;

  /// Create a copy of InferenceError
  /// with the given fields replaced by the non-null parameter values.
  @JsonKey(includeFromJson: false, includeToJson: false)
  _$$InferenceError_FormatDetectionImplCopyWith<
          _$InferenceError_FormatDetectionImpl>
      get copyWith => throw _privateConstructorUsedError;
}
