// This file is automatically generated, so please do not edit it.
// @generated by `flutter_rust_bridge`@ 2.11.1.

// ignore_for_file: invalid_use_of_internal_member, unused_import, unnecessary_import

import '../frb_generated.dart';
import '../models/error.dart';
import '../models/tensor.dart';
import 'package:flutter_rust_bridge/flutter_rust_bridge_for_generated.dart';

// These functions are ignored because they are not marked as `pub`: `create_session_info`, `download_model_progress_stream`, `download_model_with_progress_callback`, `download_model_with_progress`, `download_model`, `get_cache_dir`, `load_from_cache`, `parse_data_type`, `parse_engine_type`, `save_to_cache`, `url_to_cache_key`
// These types are ignored because they are neither used by any `pub` functions nor (for structs and enums) marked `#[frb(unignore)]`: `DownloadPhase`, `DownloadProgress`
// These function are ignored because they are on traits that is not defined in current crate (put an empty `#[frb]` on it to unignore): `clone`, `clone`, `clone`, `clone`, `clone`, `clone`, `fmt`, `fmt`, `fmt`, `fmt`, `fmt`, `fmt`
// These functions have error during generation (see debug logs or enable `stop_on_error: true` for more details): `download_progress_stream`

/// Load a model with automatic engine detection
Future<SessionInfo> loadModel({required String modelPath}) =>
    RustLib.instance.api.crateApiInferenceLoadModel(modelPath: modelPath);

/// Load a model with specific configuration
Future<SessionInfo> loadModelWithConfig(
        {required String modelPath, required SessionConfig config}) =>
    RustLib.instance.api.crateApiInferenceLoadModelWithConfig(
        modelPath: modelPath, config: config);

/// Load a model from bytes
Future<SessionInfo> loadModelFromBytes(
        {required List<int> modelBytes, required SessionConfig config}) =>
    RustLib.instance.api.crateApiInferenceLoadModelFromBytes(
        modelBytes: modelBytes, config: config);

/// Load a model with explicit Candle engine
Future<SessionInfo> loadModelWithCandle({required String modelPath}) =>
    RustLib.instance.api
        .crateApiInferenceLoadModelWithCandle(modelPath: modelPath);

/// Train a Linfa model
Future<SessionInfo> trainLinfaModel(
        {required List<Float64List> features,
        required String algorithm,
        required Map<String, String> params}) =>
    RustLib.instance.api.crateApiInferenceTrainLinfaModel(
        features: features, algorithm: algorithm, params: params);

/// Make a prediction with a loaded model
Future<InferenceResult> predict(
        {required BigInt sessionHandle, required InferenceInput input}) =>
    RustLib.instance.api
        .crateApiInferencePredict(sessionHandle: sessionHandle, input: input);

/// Make batch predictions
Future<List<InferenceResult>> predictBatch(
        {required BigInt sessionHandle,
        required List<InferenceInput> inputs}) =>
    RustLib.instance.api.crateApiInferencePredictBatch(
        sessionHandle: sessionHandle, inputs: inputs);

/// Get session information
Future<SessionInfo> getSessionInfo({required BigInt sessionHandle}) =>
    RustLib.instance.api
        .crateApiInferenceGetSessionInfo(sessionHandle: sessionHandle);

/// Dispose of a session and free resources
Future<void> disposeSession({required BigInt sessionHandle}) =>
    RustLib.instance.api
        .crateApiInferenceDisposeSession(sessionHandle: sessionHandle);

/// Get list of available engines
List<String> getAvailableEngines() =>
    RustLib.instance.api.crateApiInferenceGetAvailableEngines();

/// Check if a specific engine is available
bool isEngineAvailable({required String engineType}) => RustLib.instance.api
    .crateApiInferenceIsEngineAvailable(engineType: engineType);

/// Detect engine type from file path
String detectEngineFromPath({required String modelPath}) => RustLib.instance.api
    .crateApiInferenceDetectEngineFromPath(modelPath: modelPath);

/// Detect engine type from bytes
String detectEngineFromBytes({required List<int> modelBytes}) =>
    RustLib.instance.api
        .crateApiInferenceDetectEngineFromBytes(modelBytes: modelBytes);

/// Load a model from a URL with caching
Future<SessionInfo> loadModelFromUrl(
        {required String url, required bool cache, String? cacheKey}) =>
    RustLib.instance.api.crateApiInferenceLoadModelFromUrl(
        url: url, cache: cache, cacheKey: cacheKey);

/// Load a model from local file path
Future<SessionInfo> loadModelFromFile({required String filePath}) =>
    RustLib.instance.api.crateApiInferenceLoadModelFromFile(filePath: filePath);

/// HuggingFace integration - load model from hub using real hf-hub crate
Future<SessionInfo> loadFromHuggingface(
        {required String repo, String? revision, String? filename}) =>
    RustLib.instance.api.crateApiInferenceLoadFromHuggingface(
        repo: repo, revision: revision, filename: filename);

/// Clear model cache
Future<void> clearCache() => RustLib.instance.api.crateApiInferenceClearCache();

/// Get cache size in bytes
Future<BigInt> getCacheSize() =>
    RustLib.instance.api.crateApiInferenceGetCacheSize();

/// Input data for inference
class InferenceInput {
  final Float32List data;
  final Uint64List shape;
  final String dataType;

  const InferenceInput({
    required this.data,
    required this.shape,
    required this.dataType,
  });

  @override
  int get hashCode => data.hashCode ^ shape.hashCode ^ dataType.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is InferenceInput &&
          runtimeType == other.runtimeType &&
          data == other.data &&
          shape == other.shape &&
          dataType == other.dataType;
}

/// Result from inference
class InferenceResult {
  final Float32List data;
  final Uint64List shape;
  final String dataType;

  const InferenceResult({
    required this.data,
    required this.shape,
    required this.dataType,
  });

  @override
  int get hashCode => data.hashCode ^ shape.hashCode ^ dataType.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is InferenceResult &&
          runtimeType == other.runtimeType &&
          data == other.data &&
          shape == other.shape &&
          dataType == other.dataType;
}

/// Configuration for inference sessions
class SessionConfig {
  final String? engineType;
  final bool gpuAcceleration;
  final BigInt? numThreads;
  final String? optimizationLevel;

  const SessionConfig({
    this.engineType,
    required this.gpuAcceleration,
    this.numThreads,
    this.optimizationLevel,
  });

  static Future<SessionConfig> default_() =>
      RustLib.instance.api.crateApiInferenceSessionConfigDefault();

  @override
  int get hashCode =>
      engineType.hashCode ^
      gpuAcceleration.hashCode ^
      numThreads.hashCode ^
      optimizationLevel.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is SessionConfig &&
          runtimeType == other.runtimeType &&
          engineType == other.engineType &&
          gpuAcceleration == other.gpuAcceleration &&
          numThreads == other.numThreads &&
          optimizationLevel == other.optimizationLevel;
}

/// Session information
class SessionInfo {
  final BigInt handle;
  final String engineType;
  final List<TensorSpec> inputSpecs;
  final List<TensorSpec> outputSpecs;

  const SessionInfo({
    required this.handle,
    required this.engineType,
    required this.inputSpecs,
    required this.outputSpecs,
  });

  @override
  int get hashCode =>
      handle.hashCode ^
      engineType.hashCode ^
      inputSpecs.hashCode ^
      outputSpecs.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is SessionInfo &&
          runtimeType == other.runtimeType &&
          handle == other.handle &&
          engineType == other.engineType &&
          inputSpecs == other.inputSpecs &&
          outputSpecs == other.outputSpecs;
}
